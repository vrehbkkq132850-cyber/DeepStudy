import os
from typing import List, Dict

# --- 1. LlamaIndex æ ¸å¿ƒç»„ä»¶ ---
from llama_index.core import (
    VectorStoreIndex, 
    Document, 
    StorageContext, 
    load_index_from_storage, 
    Settings
)

# --- 2. å…³é”®ï¼šå¤åˆ»ä½ æˆªå›¾é‡Œçš„ ModelScope é…ç½® ---
# æˆ‘ä»¬å¼•ç”¨ LlamaIndex çš„ OpenAI ç±»ï¼Œä½†é€šè¿‡ä¿®æ”¹å‚æ•°è®©å®ƒå»è¿é­”æ­
from llama_index.llms.openai import OpenAI

# å¼•ç”¨ HuggingFace åµŒå…¥æ¨¡å‹ï¼ˆæœ¬åœ°è¿è¡Œï¼Œå…è´¹ä¸”å¿«ï¼‰
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# å¯¼å…¥ä½ çš„é…ç½® (ç¡®ä¿ settings é‡Œæœ‰ API KEY)
from backend.config import settings 

class VectorStoreManager:
    """
    DeepFocus å‘é‡çŸ¥è¯†åº“ç®¡ç†å™¨
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. LLM: ä½¿ç”¨ ModelScope (Qwen2.5) -> å¯¹åº”æˆªå›¾é€»è¾‘
    2. Embedding: ä½¿ç”¨æœ¬åœ° HuggingFace -> è´Ÿè´£æŠŠæ–‡å­—å˜å‘é‡
    3. Index: è´Ÿè´£å­˜å–å’Œæœç´¢
    """
    
    def __init__(self):
        # æ•°æ®çš„æœ¬åœ°å­˜å‚¨è·¯å¾„
        self.persist_dir = "./local_storage"
        
        # =====================================================
        # ğŸ‘‡ è¿™é‡Œå°±æ˜¯ä½ æˆªå›¾é‡Œçš„é€»è¾‘å¤åˆ» ğŸ‘‡
        # =====================================================
        # è™½ç„¶ç±»åå« OpenAIï¼Œä½†æˆ‘ä»¬æŠŠ api_base æ”¹æˆäº†é­”æ­çš„åœ°å€
        # è¿™å°±ç›¸å½“äºï¼šself.client = OpenAI(base_url="https://api-inference.modelscope.cn/v1/...")
        model_scope_llm = OpenAI(
            model="Qwen/Qwen2.5-Coder-32B-Instruct",  # æˆªå›¾åŒæ¬¾æ¨¡å‹
            api_key="ms-9b769e50-465c-4108-b47e-dc40e7bf22fd",      # å¡«ä½ çš„ Token
            api_base="https://api-inference.modelscope.cn/v1", # æˆªå›¾åŒæ¬¾åœ°å€
            temperature=0.1,
            max_tokens=2048
        )
        
        # å°†å…¶è®¾ç½®ä¸ºå…¨å±€é»˜è®¤ LLM
        Settings.llm = model_scope_llm
        
        # =====================================================
        # é…ç½®åµŒå…¥æ¨¡å‹ (ç”¨æ¥è®¡ç®—ç›¸ä¼¼åº¦)
        # =====================================================
        # ä½¿ç”¨æœ¬åœ°è½»é‡çº§æ¨¡å‹ï¼Œä¸éœ€è¦è”ç½‘è°ƒ API
        # é…ç½® çœ¼ç› (Embedding) -> ä½¿ç”¨ BGE ä¸­æ–‡æ¨¡å‹ (ä¸“ç²¾ä¸­æ–‡è¯­ä¹‰)
        Settings.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-small-zh-v1.5"
        )
        
        # --- åˆå§‹åŒ–ç´¢å¼• (åŠ è½½è®°å¿†æˆ–æ–°å»º) ---
        if not os.path.exists(self.persist_dir):
            print("ğŸ“­ [VectorStore] æœ¬åœ°ä¸ºç©ºï¼Œæ­£åœ¨åˆå§‹åŒ–æ–°çŸ¥è¯†åº“...")
            self.index = VectorStoreIndex.from_documents([])
            self.index.storage_context.persist(persist_dir=self.persist_dir)
        else:
            print("ğŸ“‚ [VectorStore] å‘ç°æœ¬åœ°è®°å¿†ï¼Œæ­£åœ¨åŠ è½½...")
            storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
            self.index = load_index_from_storage(storage_context)

    async def add_document(self, text: str, metadata: Dict = None):
        """
        å¯¹åº”åŠŸèƒ½ç‚¹ï¼šã€å®ç°æ–‡æœ¬ç‰‡æ®µæå–å’Œæ ‡è®°ã€‘
        LlamaIndex ä¼šè‡ªåŠ¨æŠŠ text åˆ‡åˆ†æˆç‰‡æ®µ (Chunk)ï¼Œå¹¶è®¡ç®—å‘é‡å­˜å…¥ã€‚
        """
        if not text: return

        print(f"ğŸ“¥ [å­˜å…¥] å¤„ç†ä¸­: {text[:20]}...")
        
        # å°è£…ä¸ºæ–‡æ¡£å¯¹è±¡
        doc = Document(text=text, metadata=metadata or {})
        
        # æ’å…¥ç´¢å¼•
        self.index.insert(doc)
        
        # ğŸ’¾ åªæœ‰æ‰§è¡Œè¿™ä¸€æ­¥ï¼Œé‡å¯åæ•°æ®æ‰ä¸ä¼šä¸¢
        self.index.storage_context.persist(persist_dir=self.persist_dir)
        print("âœ… [å­˜å…¥] æˆåŠŸå¹¶å·²ä¿å­˜åˆ°ç¡¬ç›˜ã€‚")

    async def search_context(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        å¯¹åº”åŠŸèƒ½ç‚¹ï¼šã€å®Œå–„åˆ’è¯è¿½é—®ä¸Šä¸‹æ–‡è·å–ã€‘
        ç”¨æˆ·é—® -> æ‰¾ç›¸å…³ç‰‡æ®µ -> è¿”å›
        """
        # åˆ›å»ºæ£€ç´¢å™¨ (Retriever)
        retriever = self.index.as_retriever(similarity_top_k=top_k)
        
        # æ‰§è¡Œæ£€ç´¢ (åŒæ­¥æ“ä½œ)
        nodes = retriever.retrieve(query)
        
        results = []
        for node in nodes:
            results.append({
                "text": node.text,          # åŸæ–‡
                "score": node.score,        # ç›¸ä¼¼åº¦ (1.0æœ€é«˜)
                "source": node.metadata.get("source", "unknown")
            })
            
        return results

# å…¨å±€å•ä¾‹ä¾›å¤–éƒ¨è°ƒç”¨
vector_store_manager = VectorStoreManager()

# --- ğŸ‘‡ æŠŠæ–‡ä»¶æœ€ä¸‹é¢çš„æµ‹è¯•ä»£ç æ”¹æˆè¿™æ · ğŸ‘‡ ---
if __name__ == "__main__":
    import asyncio
    
    async def test():
        v = VectorStoreManager()
        
        print("\n--- 1. æ­£åœ¨ç»™ AI çŒè¾“ä¸¤æ®µä¸ç›¸å…³çš„è®°å¿† ---")
        
        # è®°å¿† Aï¼šå…³äº AI çš„
        text_ai = "Qwen2.5-Coder æ˜¯é˜¿é‡Œæ¨å‡ºçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œæ“…é•¿å†™ Python å’Œ C++ã€‚"
        await v.add_document(text_ai, metadata={"source": "AI News"})
        
        # è®°å¿† Bï¼šå…³äºåšèœçš„ (å¹²æ‰°é¡¹)
        text_food = "å®«ä¿é¸¡ä¸æ˜¯ä¸€é“è‘—åçš„å·èœï¼Œä¸»è¦é€šè¿‡çˆ†ç‚’é¸¡ä¸å’ŒèŠ±ç”Ÿç±³åˆ¶æˆï¼Œå£å‘³ç³Šè¾£è”æå‘³ã€‚"
        await v.add_document(text_food, metadata={"source": "é£Ÿè°±"})
        
        print("âœ… è®°å¿†çŒè¾“å®Œæ¯•ï¼")
        
        # --- 2. è§è¯å¥‡è¿¹çš„æ—¶åˆ» ---
        
        # æé—® 1ï¼šé—®åƒçš„
        query1 = "æˆ‘æƒ³å­¦åšèœï¼Œæœ‰ä»€ä¹ˆæ¨èï¼Ÿ" 
        # æ³¨æ„ï¼šè¿™å¥è¯é‡Œå®Œå…¨æ²¡æœ‰â€œå®«ä¿é¸¡ä¸â€è¿™å››ä¸ªå­—ï¼
        
        print(f"\nâ“ ç”¨æˆ·é—®: {query1}")
        results = await v.search_context(query1, top_k=1)
        print(f"ğŸ¤– AI å›ç­”çš„å‚è€ƒèµ„æ–™: {results[0]['text']}")
        
        # æé—® 2ï¼šé—®ä»£ç 
        query2 = "å“ªä¸ªæ¨¡å‹å†™ä»£ç æ¯”è¾ƒå¥½ï¼Ÿ"
        
        print(f"\nâ“ ç”¨æˆ·é—®: {query2}")
        results = await v.search_context(query2, top_k=1)
        print(f"ğŸ¤– AI å›ç­”çš„å‚è€ƒèµ„æ–™: {results[0]['text']}")

    asyncio.run(test())